{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql as db\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import threading as t\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db.install_as_MySQLdb()\n",
    "\n",
    "import MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#출고 데이터 저장 딕셔너리\n",
    "df = 0\n",
    "#현재 날짜 가지고 오기\n",
    "now = datetime.datetime.now()\n",
    "#현재 날짜를 년,월,요일 등으로 분리\n",
    "now_result = now.timetuple()\n",
    "#요일\n",
    "weekday = now_result.tm_wday\n",
    "#월\n",
    "month = now_result.tm_mon\n",
    "#문구류_List\n",
    "Stationery_list = []\n",
    "#학습결과\n",
    "result_dict = {\"Stationery_NO\" : [], \"Count\" : [], \"Today\" : []}\n",
    "result_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DB_Connect:\n",
    "    conn = db.connect(host='localhost', user='root', password='1234', db='ai_db', charset='utf8')\n",
    "    curs = conn.cursor(db.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB_SQL\n",
    "class DB_SQL(DB_Connect):\n",
    "    #DB값 읽기\n",
    "    def db_read(self,sql):\n",
    "        print(sql)\n",
    "        conn = db.connect(host='localhost', user='root', password='1234', db='ai_db', charset='utf8')\n",
    "        curs = conn.cursor(db.cursors.DictCursor)\n",
    "        curs.execute(sql)\n",
    "        print(\"db_read 성공!\")\n",
    "        result = curs.fetchall()\n",
    "        df = pd.DataFrame(result)\n",
    "        #문구류의 값 리스트에 넣어서 중복 제거\n",
    "        Stationery_list = set(df['Stationery_NO'])\n",
    "        curs.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return df, Stationery_list\n",
    "    \n",
    "    #DB값 쓰기\n",
    "    def db_write(self,data):\n",
    "        engine = create_engine(\"mysql+mysqldb://root:\" + \"1234\" + \"@localhost:3306/ai_db\", encoding='utf-8')\n",
    "        conn = engine.connect()\n",
    "        data\n",
    "        data.to_sql(name = \"training_results\", con = engine, if_exists='append', index=False)\n",
    "        print('테이블 데이터 업데이트 완료!')\n",
    "    \n",
    "    #학습값 제거\n",
    "    def db_del(self,sql):\n",
    "        print(sql)\n",
    "        conn = db.connect(host='localhost', user='root', password='1234', db='ai_db', charset='utf8')\n",
    "        curs = conn.cursor()\n",
    "        curs.execute(sql)\n",
    "        print(\"DB_Data 제거 완료!\")\n",
    "        curs.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB_Data_Read\n",
    "def data_read(weekday):\n",
    "    db_result = DB_SQL()\n",
    "    \n",
    "    print(weekday)\n",
    "    \n",
    "    if(weekday == 4 or weekday == 6 or weekday == 5):\n",
    "        print(\"금요일 또는 토요일, 일요일은 휴일입니다.\")\n",
    "        weekday = 0\n",
    "    else:\n",
    "        weekday = weekday + 1\n",
    "        \n",
    "    sql = \"select Stationery_NO, Count, MONTH(Date) as 'months' from ai_db.releases where Weekday = \" + str(weekday) + \" and MONTH(Date) = \" +  str(month) + \";\"       \n",
    "\n",
    "    global df\n",
    "    global Stationery_list\n",
    "\n",
    "    df,Stationery_list = db_result.db_read(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습\n",
    "class AI_Model():\n",
    "    #데이터셋 변환 과정\n",
    "    def create_dataset(data, look_back=1):\n",
    "        x_arr, y_arr = [], []\n",
    "        \n",
    "        for i in range(len(data) - look_back):\n",
    "            x_arr.append(data[i:(i + look_back),0])\n",
    "            y_arr.append(data[i + look_back, 0])\n",
    "        \n",
    "        x_arr = np.array(x_arr)\n",
    "        x_arr = np.reshape(x_arr, (x_arr.shape[0], x_arr.shape[1],1))\n",
    "        \n",
    "        return x_arr, np.array(y_arr)\n",
    "    \n",
    "    #학습\n",
    "    def training(no):\n",
    "        #데이터 전처리 (0 ~ 1 범위)\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        #지금 전체 카운트롤 계산 했지만 Thread 문구류 각각해서 동시에 돌릴 예정\n",
    "        data_temp = (df['Stationery_NO'] == no)\n",
    "        data_result = df[data_temp]\n",
    "        print(data_result)\n",
    "        \n",
    "        temp = df[data_temp].values.reshape(-1,1)\n",
    "        trade_count = scaler.fit_transform(temp)\n",
    "        \n",
    "        #훈련\n",
    "        train = trade_count[0:int(len(trade_count) * 0.5)]\n",
    "        #검증\n",
    "        val = trade_count[int(len(trade_count) * 0.5) : int(len(trade_count) *0.75)]\n",
    "        #시험\n",
    "        test = trade_count[int(len(trade_count) * 0.75) : -1]\n",
    "        \n",
    "        #x,y 트레이닝 셋\n",
    "        x_train, y_train = AI_Model.create_dataset(train, 1)\n",
    "        #x,y 검증셋\n",
    "        x_val, y_val = AI_Model.create_dataset(val,1)\n",
    "        #x,y 시험 셋\n",
    "        x_test, y_text = AI_Model.create_dataset(val,1)\n",
    "        \n",
    "        #학습 모델 구성\n",
    "        batch_size = 22\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(LSTM(50, return_sequences=True, input_shape=(1,1)))\n",
    "        model.add(LSTM(64,return_sequences=False))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        #손실\n",
    "        model.compile(loss='mse', optimizer='rmsprop')\n",
    "        model.summary()\n",
    "        model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=50)\n",
    "        predictions = model.predict(x_test, batch_size)\n",
    "        read_predictions = scaler.inverse_transform(predictions)\n",
    "        result = read_predictions[-1]\n",
    "        \n",
    "        print(str(no) + \"의 문구류의 값은 \" + str(result) +\" 입니다.\")\n",
    "        \n",
    "        #학습결과_Dict\n",
    "        global result_dict\n",
    "        result_dict['Stationery_NO'].append(no)\n",
    "        result_dict['Count'].append(int(result))\n",
    "        result_now = str(now_result.tm_year) + \"-\" + str(now_result.tm_mon) + \"-\" + str(now_result.tm_mday)\n",
    "        result_dict['Today'].append(result_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#쓰레드 생성, 실행 함수\n",
    "def run_threads(list_temp):\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(len(list_temp)):\n",
    "        thread = t.Thread(target=AI_Model.training,kwargs={\"no\" : list_temp[i]})\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "select Stationery_NO, Count, MONTH(Date) as 'months' from ai_db.releases where Weekday = 4 and MONTH(Date) = 10;\n",
      "db_read 성공!\n"
     ]
    }
   ],
   "source": [
    "data_read(weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Count  Stationery_NO  months\n",
      "10        2              1      10\n",
      "11       20              1      10\n",
      "13       12              1      10\n",
      "16       25              1      10\n",
      "23        1              1      10\n",
      "34       20              1      10\n",
      "44        3              1      10\n",
      "48       41              1      10\n",
      "49        9              1      10\n",
      "68       11              1      10\n",
      "83       11              1      10\n",
      "86        2              1      10\n",
      "101      38              1      10\n",
      "102      11              1      10\n",
      "103      11              1      10\n",
      "104       6              1      10\n",
      "107      34              1      10\n",
      "109      10              1      10\n",
      "110       7              1      10\n",
      "114       1              1      10\n",
      "125      43              1      10\n",
      "129      13              1      10\n",
      "131      13              1      10\n",
      "146       2              1      10\n",
      "160       7              1      10\n",
      "161       2              1      10\n",
      "173      28              1      10\n",
      "174      14              1      10\n",
      "192       1              1      10\n",
      "196       1              1      10\n",
      "...     ...            ...     ...\n",
      "1016     33              1      10\n",
      "1022      9              1      10\n",
      "1026     45              1      10\n",
      "1027      3              1      10\n",
      "1028      4              1      10\n",
      "1031      1              1      10\n",
      "1044     16              1      10\n",
      "1046     14              1      10\n",
      "1055      3              1      10\n",
      "1056     15              1      10\n",
      "1066     35              1      10\n",
      "1091     21              1      10\n",
      "1113      3              1      10\n",
      "1114     14              1      10\n",
      "1117     13              1      10\n",
      "1146     82              1      10\n",
      "1147      2              1      10\n",
      "1151      6              1      10\n",
      "1155      1              1      10\n",
      "1160     15              1      10\n",
      "1165     42              1      10\n",
      "1173      7              1      10\n",
      "1177      4              1      10\n",
      "1191     17              1      10\n",
      "1198     12              1      10\n",
      "1209      4              1      10\n",
      "1217      1              1      10\n",
      "1222      1              1      10\n",
      "1229     37              1      10\n",
      "1231      5              1      10\n",
      "\n",
      "[187 rows x 3 columns]\n",
      "      Count  Stationery_NO  months\n",
      "5         9              2      10\n",
      "6         1              2      10\n",
      "8        14              2      10\n",
      "12        4              2      10\n",
      "24        6              2      10\n",
      "25        5              2      10\n",
      "26        7              2      10\n",
      "27        1              2      10\n",
      "33       31              2      10\n",
      "36        2              2      10\n",
      "50       13              2      10\n",
      "54        4              2      10\n",
      "55       10              2      10\n",
      "56        2              2      10\n",
      "62        9              2      10\n",
      "67       52              2      10\n",
      "80        9              2      10\n",
      "85       34              2      10\n",
      "90        2              2      10\n",
      "95       14              2      10\n",
      "100       1              2      10\n",
      "105       1              2      10\n",
      "151      21              2      10\n",
      "152      16              2      10\n",
      "153      10              2      10\n",
      "157       5              2      10\n",
      "163      31              2      10\n",
      "166      48              2      10\n",
      "167       7              2      10\n",
      "175      16              2      10\n",
      "...     ...            ...     ...\n",
      "1077      5              2      10\n",
      "1079      2              2      10\n",
      "1083     17              2      10\n",
      "1087      7              2      10\n",
      "1088      1              2      10\n",
      "1096     24              2      10\n",
      "1099     10              2      10\n",
      "1121      5              2      10\n",
      "1126     21              2      10\n",
      "1127      3              2      10\n",
      "1132      1              2      10\n",
      "1133     26              2      10\n",
      "1142      4              2      10\n",
      "1148      1              2      10\n",
      "1158     18              2      10\n",
      "1163      3              2      10\n",
      "1167      3              2      10\n",
      "1168      1              2      10\n",
      "1180     26              2      10\n",
      "1186      1              2      10\n",
      "1196     50              2      10\n",
      "1197      9              2      10\n",
      "1199      1              2      10\n",
      "1204     12              2      10\n",
      "1208     11              2      10\n",
      "1216      5              2      10\n",
      "1218     16              2      10\n",
      "1220      1              2      10\n",
      "1232     17              2      10\n",
      "1234      9              2      10\n",
      "\n",
      "[201 rows x 3 columns]\n",
      "      Count  Stationery_NO  months\n",
      "3        63              3      10\n",
      "4        24              3      10\n",
      "14       17              3      10\n",
      "15       16              3      10\n",
      "17        6              3      10\n",
      "40       37              3      10\n",
      "41        8              3      10\n",
      "45        2              3      10\n",
      "52        2              3      10\n",
      "53        1              3      10\n",
      "66       57              3      10\n",
      "75        1              3      10\n",
      "87        5              3      10\n",
      "92       26              3      10\n",
      "98        6              3      10\n",
      "106      27              3      10\n",
      "111       4              3      10\n",
      "122      21              3      10\n",
      "127      48              3      10\n",
      "137       5              3      10\n",
      "138       2              3      10\n",
      "140       7              3      10\n",
      "141       1              3      10\n",
      "144      14              3      10\n",
      "148       9              3      10\n",
      "150       1              3      10\n",
      "164      31              3      10\n",
      "169      17              3      10\n",
      "171       3              3      10\n",
      "183       3              3      10\n",
      "...     ...            ...     ...\n",
      "1010      4              3      10\n",
      "1011      1              3      10\n",
      "1037      6              3      10\n",
      "1040      9              3      10\n",
      "1058     14              3      10\n",
      "1063     71              3      10\n",
      "1073      1              3      10\n",
      "1074      3              3      10\n",
      "1092      4              3      10\n",
      "1093     15              3      10\n",
      "1102      6              3      10\n",
      "1109      1              3      10\n",
      "1116      8              3      10\n",
      "1118      2              3      10\n",
      "1136     29              3      10\n",
      "1137      6              3      10\n",
      "1140      2              3      10\n",
      "1144      8              3      10\n",
      "1150      1              3      10\n",
      "1159     25              3      10\n",
      "1161      8              3      10\n",
      "1170      1              3      10\n",
      "1172      5              3      10\n",
      "1176      1              3      10\n",
      "1188     50              3      10\n",
      "1193      5              3      10\n",
      "1194      1              3      10\n",
      "1205     16              3      10\n",
      "1207      9              3      10\n",
      "1219     12              3      10\n",
      "\n",
      "[205 rows x 3 columns]\n",
      "      Count  Stationery_NO  months\n",
      "0         4              5      10\n",
      "2         3              5      10\n",
      "21       23              5      10\n",
      "28       52              5      10\n",
      "29        2              5      10\n",
      "30        1              5      10\n",
      "31       22              5      10\n",
      "32        4              5      10\n",
      "35        8              5      10\n",
      "37       75              5      10\n",
      "38        1              5      10\n",
      "39        3              5      10\n",
      "51        2              5      10\n",
      "58        7              5      10\n",
      "65        5              5      10\n",
      "71       14              5      10\n",
      "72       26              5      10\n",
      "73        4              5      10\n",
      "74        6              5      10\n",
      "77        1              5      10\n",
      "78        3              5      10\n",
      "82       24              5      10\n",
      "88       31              5      10\n",
      "96       20              5      10\n",
      "99        1              5      10\n",
      "112      13              5      10\n",
      "123      51              5      10\n",
      "149       5              5      10\n",
      "154      60              5      10\n",
      "155      12              5      10\n",
      "...     ...            ...     ...\n",
      "1062      1              5      10\n",
      "1081     26              5      10\n",
      "1089      3              5      10\n",
      "1100     21              5      10\n",
      "1103     14              5      10\n",
      "1105      2              5      10\n",
      "1106      1              5      10\n",
      "1110     15              5      10\n",
      "1111     13              5      10\n",
      "1112      3              5      10\n",
      "1115     37              5      10\n",
      "1122     39              5      10\n",
      "1128      1              5      10\n",
      "1138     48              5      10\n",
      "1143      4              5      10\n",
      "1152      2              5      10\n",
      "1153      2              5      10\n",
      "1157      2              5      10\n",
      "1178     31              5      10\n",
      "1179     42              5      10\n",
      "1181      1              5      10\n",
      "1184      1              5      10\n",
      "1185      2              5      10\n",
      "1187      2              5      10\n",
      "1190     37              5      10\n",
      "1192      8              5      10\n",
      "1206     24              5      10\n",
      "1212      8              5      10\n",
      "1228      5              5      10\n",
      "1235      6              5      10\n",
      "\n",
      "[215 rows x 3 columns]      Count  Stationery_NO  months\n",
      "7        17              4      10\n",
      "9         1              4      10\n",
      "18       20              4      10\n",
      "19        4              4      10\n",
      "20        3              4      10\n",
      "47        3              4      10\n",
      "57        1              4      10\n",
      "63        3              4      10\n",
      "64       29              4      10\n",
      "69       10              4      10\n",
      "70       22              4      10\n",
      "81       61              4      10\n",
      "84        4              4      10\n",
      "89        1              4      10\n",
      "91        1              4      10\n",
      "93       67              4      10\n",
      "97        2              4      10\n",
      "115      22              4      10\n",
      "116      10              4      10\n",
      "118       1              4      10\n",
      "121       1              4      10\n",
      "128       1              4      10\n",
      "132      16              4      10\n",
      "133       1              4      10\n",
      "134       2              4      10\n",
      "135      45              4      10\n",
      "136       2              4      10\n",
      "139       1              4      10\n",
      "143       6              4      10\n",
      "145      28              4      10\n",
      "...     ...            ...     ...\n",
      "1082      8              4      10\n",
      "1086      1              4      10\n",
      "1094     17              4      10\n",
      "1098     19              4      10\n",
      "1101     11              4      10\n",
      "1107      2              4      10\n",
      "1108      5              4      10\n",
      "1124     24              4      10\n",
      "1129     15              4      10\n",
      "1130     12              4      10\n",
      "1131      3              4      10\n",
      "1139     13              4      10\n",
      "1154      5              4      10\n",
      "1156      2              4      10\n",
      "1162      8              4      10\n",
      "1164     10              4      10\n",
      "1166      1              4      10\n",
      "1169      2              4      10\n",
      "1171     17              4      10\n",
      "1175      6              4      10\n",
      "1183      6              4      10\n",
      "1189     22              4      10\n",
      "1203     28              4      10\n",
      "1210      9              4      10\n",
      "1213      8              4      10\n",
      "1214      1              4      10\n",
      "1225     40              4      10\n",
      "1226     11              4      10\n",
      "1230      1              4      10\n",
      "1233      1              4      10\n",
      "\n",
      "[212 rows x 3 columns]\n",
      "\n",
      "      Count  Stationery_NO  months\n",
      "1        20              6      10\n",
      "22       82              6      10\n",
      "42       31              6      10\n",
      "43       17              6      10\n",
      "46        3              6      10\n",
      "59       17              6      10\n",
      "60       29              6      10\n",
      "61        1              6      10\n",
      "76        1              6      10\n",
      "79        4              6      10\n",
      "94       20              6      10\n",
      "108       1              6      10\n",
      "113       1              6      10\n",
      "117       2              6      10\n",
      "119      20              6      10\n",
      "120      21              6      10\n",
      "124       3              6      10\n",
      "126      16              6      10\n",
      "130      37              6      10\n",
      "142      16              6      10\n",
      "147      15              6      10\n",
      "158       4              6      10\n",
      "168       4              6      10\n",
      "172      10              6      10\n",
      "185      26              6      10\n",
      "191       2              6      10\n",
      "193       9              6      10\n",
      "201       2              6      10\n",
      "214      10              6      10\n",
      "221       3              6      10\n",
      "...     ...            ...     ...\n",
      "1064     64              6      10\n",
      "1069      4              6      10\n",
      "1080     56              6      10\n",
      "1084      1              6      10\n",
      "1085     11              6      10\n",
      "1090     35              6      10\n",
      "1095      6              6      10\n",
      "1097     24              6      10\n",
      "1104      5              6      10\n",
      "1119     12              6      10\n",
      "1120      4              6      10\n",
      "1123      4              6      10\n",
      "1125      1              6      10\n",
      "1134     39              6      10\n",
      "1135     11              6      10\n",
      "1141      1              6      10\n",
      "1145      8              6      10\n",
      "1149      1              6      10\n",
      "1174     14              6      10\n",
      "1182      1              6      10\n",
      "1195     16              6      10\n",
      "1200     14              6      10\n",
      "1201      3              6      10\n",
      "1202      3              6      10\n",
      "1211     31              6      10\n",
      "1215      5              6      10\n",
      "1221      3              6      10\n",
      "1223      2              6      10\n",
      "1224      3              6      10\n",
      "1227     54              6      10\n",
      "\n",
      "[216 rows x 3 columns]\n",
      "WARNING:tensorflow:From C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 306 samples, validate on 153 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 323 samples, validate on 161 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 317 samples, validate on 158 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 279 samples, validate on 139 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 300 samples, validate on 150 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 321 samples, validate on 160 samples\n",
      "WARNING:tensorflow:From C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "317/317 [==============================] - 6s 19ms/sample - loss: 0.0280 - val_loss: 0.0110\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 6s 19ms/sample - loss: 0.0250 - val_loss: 0.0191\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0213 - val_loss: 0.0188\n",
      "176/323 [===============>..............] - ETA: 5s - loss: 0.0230Epoch 3/50\n",
      "317/317 [==============================] - 0s 1ms/sample - loss: 0.0235 - val_loss: 0.0113\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0211 - val_loss: 0.018823 [=======================>......] - ETA: 1s - loss\n",
      "Epoch 4/50\n",
      "317/317 [==============================] - 0s 962us/sample - loss: 0.0232 - val_loss: 0.0114\n",
      "Epoch 4/50\n",
      "279/279 [==============================] - 7s 24ms/sample - loss: 0.0193 - val_loss: 0.0156\n",
      "\bEpoch 2/50\n",
      "300/300 [==============================] - 0s 782us/sample - loss: 0.0210 - val_loss: 0.0188\n",
      "Epoch 5/50\n",
      "323/323 [==============================] - 7s 21ms/sample - loss: 0.0179 - val_loss: 0.0103\n",
      "Epoch 2/50\n",
      "317/317 [==============================] - 0s 891us/sample - loss: 0.0231 - val_loss: 0.0112\n",
      "Epoch 5/50\n",
      "321/321 [==============================] - 7s 21ms/sample - loss: 0.0241 - val_loss: 0.0118\n",
      "Epoch 2/50\n",
      "306/306 [==============================] - 7s 22ms/sample - loss: 0.0261 - val_loss: 0.0141\n",
      "Epoch 2/50\n",
      "279/279 [==============================] - 0s 1ms/sample - loss: 0.0161 - val_loss: 0.0151\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s 1ms/sample - loss: 0.0208 - val_loss: 0.0186\n",
      "Epoch 6/50\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0220 - val_loss: 0.0137\n",
      "Epoch 3/50\n",
      "323/323 [==============================] - 0s 1ms/sample - loss: 0.0146 - val_loss: 0.0101\n",
      "Epoch 3/50\n",
      "321/321 [==============================] - 0s 2ms/sample - loss: 0.0196 - val_loss: 0.0115\n",
      "Epoch 3/50\n",
      "317/317 [==============================] - 0s 2ms/sample - loss: 0.0230 - val_loss: 0.0118\n",
      " 22/323 [=>............................] - ETA: 0s - loss: 0.0023Epoch 6/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0158 - val_loss: 0.0150\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0208 - val_loss: 0.0186s - loss: \n",
      "Epoch 7/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0217 - val_loss: 0.0136\n",
      "Epoch 4/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0145 - val_loss: 0.0101\n",
      "Epoch 4/50\n",
      "242/279 [=========================>....] - ETA: 0s - loss: 0.0144317/317 [==============================] - 1s 2ms/sample - loss: 0.0231 - val_loss: 0.0116\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0192 - val_loss: 0.0115\n",
      "Epoch 7/50\n",
      "Epoch 4/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0157 - val_loss: 0.0150\n",
      "154/306 [==============>...............] - ETA: 0s - loss: 0.0255Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0190==================>..........] - ETA: 0s - loss: 0.022264/323 [=======================>......] - ETA: 0s - loss: 0.\n",
      "Epoch 8/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0215 - val_loss: 0.0135\n",
      "Epoch 5/50\n",
      "323/323 [==============================] - 0s 1ms/sample - loss: 0.0145 - val_loss: 0.0101\n",
      "Epoch 5/50\n",
      "317/317 [==============================] - 0s 2ms/sample - loss: 0.0228 - val_loss: 0.0117\n",
      "Epoch 8/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      " 22/317 [=>............................] - ETA: 0s - loss: 0.0185Epoch 5/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0157 - val_loss: 0.0149\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0189\n",
      "Epoch 9/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0215 - val_loss: 0.0135\n",
      "Epoch 6/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0101\n",
      "Epoch 6/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0110\n",
      "Epoch 9/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0157 - val_loss: 0.0148\n",
      "Epoch 7/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0208 - val_loss: 0.0187..............] - ETA: 0s  - ETA: 0s - loss: 0\n",
      "Epoch 10/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0214 - val_loss: 0.0134\n",
      "Epoch 7/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0145 - val_loss: 0.0102\n",
      "Epoch 7/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0156 - val_loss: 0.0149\n",
      "Epoch 8/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0108\n",
      "Epoch 10/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0192 - val_loss: 0.0115==>...........] - ETA: 0s - loss: 0.0\n",
      " 66/279 [======>.......................] - ETA: 0s - loss: 0.0222Epoch 7/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "176/317 [===============>..............] - ETA: 0s - loss: 0.0171\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0212 - val_loss: 0.0143\n",
      "Epoch 8/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0156 - val_loss: 0.0147\n",
      "Epoch 9/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0102\n",
      "Epoch 8/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0117\n",
      "Epoch 11/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0114\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0204 - val_loss: 0.0198\n",
      "Epoch 12/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0215 - val_loss: 0.0137\n",
      "Epoch 9/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0147\n",
      "Epoch 10/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0101\n",
      "Epoch 9/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0112\n",
      "Epoch 12/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      " 22/317 [=>............................] - ETA: 0s - loss: 0.0050Epoch 9/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0210 - val_loss: 0.0189\n",
      "Epoch 13/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0214 - val_loss: 0.0134\n",
      "Epoch 10/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0147\n",
      "Epoch 11/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0102..............] - ETA: 0s - loss: 0.01\n",
      "Epoch 10/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0118\n",
      "Epoch 13/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "286/300 [===========================>..] - ETA: 0s - loss: 0.0213Epoch 10/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "Epoch 14/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0155 - val_loss: 0.0146\n",
      "Epoch 12/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0136\n",
      "Epoch 11/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0145 - val_loss: 0.0101\n",
      "Epoch 11/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0114\n",
      "Epoch 14/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0113\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0188\n",
      "Epoch 15/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0147\n",
      "\bEpoch 13/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 12/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0103\n",
      "Epoch 12/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0114\n",
      "Epoch 15/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0188 - val_loss: 0.0129\n",
      "279/279 [==============================] - 1s 2ms/sample - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 12/50\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0188\n",
      "Epoch 16/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0138\n",
      " 66/279 [======>.......................] - ETA: 0s - loss: 0.0103Epoch 13/5\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0155 - val_loss: 0.0147 - loss: 0.0138 - ETA: 0s - loss\n",
      "Epoch 15/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0190\n",
      "Epoch 17/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0112\n",
      "Epoch 16/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 13/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0214 - val_loss: 0.0135\n",
      "Epoch 14/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "242/306 [======================>.......] - ETA: 0s - loss: 0.0229Epoch 16/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0188\n",
      "Epoch 18/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0101\n",
      "Epoch 14/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0119\n",
      "Epoch 17/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "Epoch 14/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 15/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147========>....]154/321 [=============>................] - ETA: 0s - \n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 19/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0101\n",
      "Epoch 15/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0114\n",
      "Epoch 15/5\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0112\n",
      "Epoch 18/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0136\n",
      "Epoch 16/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0208 - val_loss: 0.0186=>............] - ETA: 0s - los\n",
      "Epoch 20/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0102\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 17/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 16/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0111\n",
      "Epoch 19/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0155 - val_loss: 0.0147\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "110/279 [==========>...................] - ETA: 0s - loss: 0.0193\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0212 - val_loss: 0.0139\n",
      "Epoch 18/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0103\n",
      "Epoch 17/50\n",
      "317/317 [==============================] - 0s 2ms/sample - loss: 0.0229 - val_loss: 0.0108\n",
      "Epoch 20/50\n",
      "321/321 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 17/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0208 - val_loss: 0.0186\n",
      "Epoch 22/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0214 - val_loss: 0.0135\n",
      "Epoch 19/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 18/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0114\n",
      "Epoch 21/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 18/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0206 - val_loss: 0.0190\n",
      "Epoch 23/50\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0213 - val_loss: 0.0134=>............] - ETA: 0s - los\n",
      "Epoch 20/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 19/50\n",
      "317/317 [==============================] - 0s 2ms/sample - loss: 0.0228 - val_loss: 0.0122\n",
      "Epoch 22/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 22/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 24/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0137\n",
      "Epoch 21/50\n",
      "279/279 [==============================] - 0s 1ms/sample - loss: 0.0154 - val_loss: 0.0146..............] - ETA: 0s - loss: 0.\n",
      "Epoch 23/50\n",
      "317/317 [==============================] - 0s 2ms/sample - loss: 0.0230 - val_loss: 0.0113\n",
      "Epoch 23/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0101\n",
      "Epoch 20/50\n",
      "321/321 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0115\n",
      " 66/279 [======>.......................] - ETA: 0s - loss: 0.0154Epoch 20/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0187===>.........] - ETA: 0s - loss: 0.01\n",
      "286/306 [===========================>..] - ETA: 0s - loss: 0.0221\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0214 - val_loss: 0.0134\n",
      "Epoch 22/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 24/50\n",
      "321/321 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0114\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0142 - val_loss: 0.0105\n",
      "Epoch 21/50\n",
      "Epoch 21/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0123\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0206 - val_loss: 0.0189\n",
      "110/317 [=========>....................] - ETA: 0s - loss: 0.0150Epoch 26/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 23/50\n",
      "279/279 [==============================] - 0s 1ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 25/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "\bEpoch 22/50\n",
      "317/317 [============================== - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0118\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0145 - val_loss: 0.0101\n",
      "Epoch 25/50Epoch 22/50\n",
      "\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 27/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0147\n",
      "154/321 [=============>................] - ETA: 0s - loss: 0.0171Epoch 26/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0136\n",
      "Epoch 24/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "Epoch 23/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0103\n",
      "Epoch 23/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0118\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0206 - val_loss: 0.0188\n",
      "Epoch 28/50\n",
      "110/321 [=========>....................] - ETA: 0s - loss: 0.0183279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 27/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0214 - val_loss: 0.0135\n",
      "Epoch 25/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114..............]\n",
      "Epoch 24/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0100\n",
      "Epoch 24/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0123\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 29/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 28/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0135\n",
      "110/323 [=========>....................] - ETA: 0s - loss: 0.0195Epoch 26/50\n",
      "321/321 [==============================] - 0s 2ms/sample - loss: 0.0190 - val_loss: 0.0115..............\n",
      "Epoch 25/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "\bEpoch 29/5\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "Epoch 25/50Epoch 30/50\n",
      "\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0109\n",
      "Epoch 28/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "110/321 [=========>....................] - ETA: 0s - loss: 0.0146Epoch 27/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146..............] \n",
      "242/306 [======================>.......] - ETA: 0s - loss: 0.0227Epoch 30/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "Epoch 31/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0121\n",
      "Epoch 26/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0103\n",
      " 22/321 [=>............................] - ETA: 0s - loss: 0.0368Epoch 26/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0111\n",
      "Epoch 29/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 28/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "Epoch 32/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 27/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 27/5\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0112\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 29/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146.............] - ETA: 0s - l\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0187=====>.......] - ETA: 0s - loss: 0.021\n",
      "Epoch 33/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 28/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 28/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0212 - val_loss: 0.0139\n",
      "Epoch 30/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0118\n",
      "Epoch 31/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0187==>...........] - ETA: 0s - loss: 0.0\n",
      "Epoch 34/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0100\n",
      "Epoch 29/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0119\n",
      "Epoch 29/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0212 - val_loss: 0.0134\n",
      "Epoch 31/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0230 - val_loss: 0.0117\n",
      "Epoch 32/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0206 - val_loss: 0.0188\n",
      "Epoch 35/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 32/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0101\n",
      "Epoch 30/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0120\n",
      "154/300 [==============>...............] - ETA: 0s - loss: 0.0162Epoch 30/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0118\n",
      "Epoch 33/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0147\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 36/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0212 - val_loss: 0.0139\n",
      "Epoch 33/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0100\n",
      "Epoch 31/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0192 - val_loss: 0.0115\n",
      "Epoch 31/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 36/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0120\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0206 - val_loss: 0.0189..............] - ETA: 0s - loss: 0.\n",
      "Epoch 37/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0214 - val_loss: 0.0135\n",
      "Epoch 34/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0148\n",
      "Epoch 37/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 32/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0116\n",
      " 22/323 [=>............................] - ETA: 0s - loss: 0.0044Epoch 32/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0111\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0189\n",
      "Epoch 38/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0214 - val_loss: 0.0135\n",
      "242/317 [=====================>........] - ETA: 0s - loss: 0.0236Epoch 35/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147\n",
      " 22/306 [=>............................] - ETA: 0s - loss: 0.0453Epoch 38/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0101\n",
      " 66/279 [======>.......................] - ETA: 0s - loss: 0.0194Epoch 33/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0113\n",
      "242/300 [=======================>......] - ETA: 0s - loss: 0.0215Epoch 33/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0112\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0187\n",
      "198/279 [====================>.........] - ETA: 0s - loss: 0.0163Epoch 39/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 39/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0212 - val_loss: 0.0134\n",
      "Epoch 36/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0100\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 34/50\n",
      "Epoch 34/50\n",
      "317/317 [==============================] - 0s 2ms/sample - loss: 0.0229 - val_loss: 0.0111\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0186=>...........] - ETA: 0s - loss: 0.02\n",
      "Epoch 40/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 40/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0135\n",
      "Epoch 37/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0115\n",
      "Epoch 35/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0100\n",
      "154/306 [==============>...............] - ETA: 0s - loss: 0.0144Epoch 35/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0112\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 41/50\n",
      "279/279 [==============================] - 1s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 41/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0214 - val_loss: 0.0134\n",
      "Epoch 38/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0115..............] - ETA: 0\n",
      "Epoch 36/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 36/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0111\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0208 - val_loss: 0.0187\n",
      " 22/317 [=>............................] - ETA: 0s - loss: 0.0337Epoch 42/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 42/50\n",
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0212 - val_loss: 0.0134\n",
      "Epoch 39/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0114..............] - ETA\n",
      "Epoch 37/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0188\n",
      "Epoch 43/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0112\n",
      "Epoch 40/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      " 66/323 [=====>........................] - ETA: 0s - loss: 0.0034Epoch 43/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0134..............] - ETA: 0s - loss: 0.02\n",
      "Epoch 40/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0117======>......] - ETA: 0s - loss:\n",
      "Epoch 38/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0102\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 44/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 44/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0109\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - 0s 2ms/sample - loss: 0.0213 - val_loss: 0.0136\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0186\n",
      "279/279 [==============================] - 1s 2ms/sample - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 45/50\n",
      "Epoch 45/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0116\n",
      "Epoch 39/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0143 - val_loss: 0.0105\n",
      "Epoch 39/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0115\n",
      "Epoch 42/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 42/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146==>...........] - ETA: 0s - loss: 0.023\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0188\n",
      "Epoch 46/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0115\n",
      "Epoch 40/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 40/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0119\n",
      "Epoch 43/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "Epoch 43/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0154 - val_loss: 0.0146>............] - ETA: 0s - loss: 0. - ETA: 0s - loss: 0.02\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0191\n",
      "Epoch 47/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 41/50\n",
      "Epoch 41/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0136\n",
      "Epoch 44/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0111\n",
      "Epoch 44/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0207 - val_loss: 0.0190=========>....] - ETA: 0s - loss: 0.019\n",
      "Epoch 48/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0101\n",
      "Epoch 42/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0114\n",
      "Epoch 42/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.0136\n",
      " 22/323 [=>............................] - ETA: 0s - loss: 0.0033Epoch 45/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0229 - val_loss: 0.0112\n",
      "Epoch 45/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "242/300 [=======================>......] - ETA: 0s - loss: 0.0226Epoch 49/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 49/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0116..............] - ETA: 0s - l\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0212 - val_loss: 0.0134\n",
      "\bEpoch 43/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 46/50\n",
      "Epoch 43/50\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0109\n",
      "Epoch 46/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 1s 2ms/sample - loss: 0.0206 - val_loss: 0.0186===========>..] - ETA: 0s - loss: 154/317 [=============>................] - ETA: 0s - loss: 0.021\n",
      "Epoch 50/50\n",
      "306/306 [==============================] - 1s 2ms/sample - loss: 0.0213 - val_loss: 0.01347 [=====================>........] - ETA: 0s - loss: 0.\n",
      "Epoch 47/50\n",
      "323/323 [==============================] - 1s 2ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 44/50\n",
      "321/321 [==============================] - 1s 2ms/sample - loss: 0.0190 - val_loss: 0.0114\n",
      " 22/323 [=>............................] - ETA: 0s - loss: 0.0081Epoch 44/50\n",
      "279/279 [==============================] - 0s 2ms/sample - loss: 0.0153 - val_loss: 0.0146\n",
      "317/317 [==============================] - 1s 2ms/sample - loss: 0.0228 - val_loss: 0.0118\n",
      " 66/323 [=====>........................] - ETA: 0s - loss: 0.0124Epoch 47/50\n",
      "300/300 [==============================] - 0s 2ms/sample - loss: 0.0208 - val_loss: 0.0186\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0213 - val_loss: 0.0134>............] - ETA: 0s - loss: 0.\n",
      "Epoch 48/50\n",
      "323/323 [==============================] - 0s 1ms/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 45/50\n",
      "321/321 [==============================] - 0s 1ms/sample - loss: 0.0189 - val_loss: 0.0113\n",
      "Epoch 45/50\n",
      "1의 문구류의 값은 [9.383149] 입니다.\n",
      "317/317 [==============================] - 0s 1ms/sample - loss: 0.0229 - val_loss: 0.0111\n",
      "Epoch 48/50\n",
      "264/306 [========================>.....] - ETA: 0s - loss: 0.0222의 문구류의 값은 [9.514521] 입니다.\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0213 - val_loss: 0.0135=====>........] - ETA: 0s - loss: 0.\n",
      "Epoch 49/50\n",
      "323/323 [==============================] - 0s 1ms/sample - loss: 0.0143 - val_loss: 0.0103\n",
      "Epoch 46/50\n",
      "321/321 [==============================] - 0s 1ms/sample - loss: 0.0191 - val_loss: 0.0114\n",
      "Epoch 46/50\n",
      "317/317 [==============================] - 0s 1ms/sample - loss: 0.0228 - val_loss: 0.0114\n",
      "Epoch 49/50\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0213 - val_loss: 0.0134\n",
      "220/317 [===================>..........] - ETA: 0s - loss: 0.0240Epoch 50/50\n",
      "323/323 [==============================] - 0s 1ms/sample - loss: 0.0143 - val_loss: 0.0104\n",
      "Epoch 47/50\n",
      "321/321 [==============================] - 0s 1ms/sample - loss: 0.0190 - val_loss: 0.0114\n",
      "Epoch 47/50\n",
      "317/317 [==============================] - 0s 1ms/sample - loss: 0.0228 - val_loss: 0.0115\n",
      "Epoch 50/50\n",
      "306/306 [==============================] - 0s 1ms/sample - loss: 0.0213 - val_loss: 0.0135\n",
      "323/323 [==============================] - 0s 1ms/sample - loss: 0.0144 - val_loss: 0.0101\n",
      "Epoch 48/50\n",
      "321/321 [==============================] - 0s 975us/sample - loss: 0.0191 - val_loss: 0.0113\n",
      "Epoch 48/50\n",
      "317/317 [==============================] - ETA: 0s - loss: 0.027 - 0s 1ms/sample - loss: 0.0229 - val_loss: 0.0112\n",
      "323/323 [==============================] - 0s 762us/sample - loss: 0.0144 - val_loss: 0.0102\n",
      "Epoch 49/50\n",
      "321/321 [==============================] - 0s 738us/sample - loss: 0.0190 - val_loss: 0.0116\n",
      "Epoch 49/50\n",
      "3의 문구류의 값은 [8.557104] 입니다.\n",
      "220/321 [===================>..........] - ETA: 0s - loss: 0.02124의 문구류의 값은 [9.098314] 입니다.\n",
      "323/323 [==============================] - 0s 697us/sample - loss: 0.0144 - val_loss: 0.0101\n",
      "Epoch 50/50\n",
      "321/321 [==============================] - 0s 689us/sample - loss: 0.0191 - val_loss: 0.0115\n",
      "Epoch 50/50\n",
      "323/323 [==============================] - 0s 613us/sample - loss: 0.0144 - val_loss: 0.0100\n",
      "321/321 [==============================] - 0s 598us/sample - loss: 0.0190 - val_loss: 0.0115\n",
      "6의 문구류의 값은 [9.675602] 입니다.\n",
      "5의 문구류의 값은 [9.954457] 입니다.\n",
      "thread time :  47.27434945106506\n"
     ]
    }
   ],
   "source": [
    "#모델 객체 생성\n",
    "ai_model = AI_Model()\n",
    "\n",
    "list_temp = list(Stationery_list)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#학습 모델 쓰레드 생성\n",
    "run_threads(list_temp)\n",
    "\n",
    "#쓰레드 미생성\n",
    "# for i in range(len(list_temp)):\n",
    "#     ai_model.training(list_temp[i])\n",
    "\n",
    "print('thread time : ', time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stationery_NO</th>\n",
       "      <th>Count</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Stationery_NO  Count       Today\n",
       "id                                  \n",
       "1               1      9  2019-10-31\n",
       "2               2      9  2019-10-31\n",
       "3               3      8  2019-10-31\n",
       "4               4      9  2019-10-31\n",
       "5               6      9  2019-10-31\n",
       "6               5      9  2019-10-31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result_dict)\n",
    "result_df.index +=1\n",
    "result_df.index.names = ['id']\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 데이터 업데이트 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:170: Warning: (1366, \"Incorrect string value: '\\\\xB4\\\\xEB\\\\xC7\\\\xD1\\\\xB9\\\\xCE...' for column 'VARIABLE_VALUE' at row 1\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "db_result = DB_SQL()\n",
    "\n",
    "sql = \"TRUNCATE TABLE training_results\"\n",
    "\n",
    "db_result.db_write(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
