{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql as db\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import threading as t\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db.install_as_MySQLdb()\n",
    "\n",
    "import MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#출고 데이터 저장 딕셔너리\n",
    "df = 0\n",
    "#현재 날짜 가지고 오기\n",
    "now = datetime.datetime.now()\n",
    "#현재 날짜를 년,월,요일 등으로 분리\n",
    "now_result = now.timetuple()\n",
    "#요일\n",
    "weekday = now_result.tm_wday\n",
    "#월\n",
    "month = now_result.tm_mon\n",
    "#문구류_List\n",
    "Stationery_list = []\n",
    "#학습결과\n",
    "result_dict = {\"Stationery_NO\" : [], \"Count\" : [], \"Today\" : []}\n",
    "result_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DB_Connect:\n",
    "    conn = db.connect(host='localhost', user='root', password='1234', db='ai_db', charset='utf8')\n",
    "    curs = conn.cursor(db.cursors.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB_SQL\n",
    "class DB_SQL(DB_Connect):\n",
    "    #DB값 읽기\n",
    "    def db_read(self,sql):\n",
    "        print(sql)\n",
    "        conn = db.connect(host='localhost', user='root', password='1234', db='ai_db', charset='utf8')\n",
    "        curs = conn.cursor(db.cursors.DictCursor)\n",
    "        curs.execute(sql)\n",
    "        print(\"db_read 성공!\")\n",
    "        result = curs.fetchall()\n",
    "        df = pd.DataFrame(result)\n",
    "        #문구류의 값 리스트에 넣어서 중복 제거\n",
    "        Stationery_list = set(df['Stationery_NO'])\n",
    "        curs.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return df, Stationery_list\n",
    "    \n",
    "    #DB값 쓰기\n",
    "    def db_write(self,data):\n",
    "        engine = create_engine(\"mysql+mysqldb://root:\" + \"1234\" + \"@localhost:3306/ai_db\", encoding='utf-8')\n",
    "        conn = engine.connect()\n",
    "        data\n",
    "        data.to_sql(name = \"training_results\", con = engine, if_exists='append', index=False)\n",
    "        print('테이블 데이터 업데이트 완료!')\n",
    "    \n",
    "    #학습값 제거\n",
    "    def db_del(self,sql):\n",
    "        print(sql)\n",
    "        conn = db.connect(host='localhost', user='root', password='1234', db='ai_db', charset='utf8')\n",
    "        curs = conn.cursor()\n",
    "        curs.execute(sql)\n",
    "        print(\"DB_Data 제거 완료!\")\n",
    "        curs.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB_Data_Read\n",
    "def data_read(weekday):\n",
    "    db_result = DB_SQL()\n",
    "    \n",
    "    print(weekday)\n",
    "    \n",
    "    if(weekday == 4 or weekday == 6 or weekday == 5):\n",
    "        print(\"금요일 또는 토요일, 일요일은 휴일입니다.\")\n",
    "        weekday = 0\n",
    "    else:\n",
    "        weekday = weekday + 1\n",
    "        \n",
    "    sql = \"select Stationery_NO, Count, MONTH(Date) as 'months' from ai_db.releases where Weekday = \" + str(weekday) + \" and MONTH(Date) = \" +  str(month) + \";\"       \n",
    "\n",
    "    global df\n",
    "    global Stationery_list\n",
    "\n",
    "    df,Stationery_list = db_result.db_read(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습\n",
    "class AI_Model():\n",
    "    #데이터셋 변환 과정\n",
    "    def create_dataset(data, look_back=1):\n",
    "        x_arr, y_arr = [], []\n",
    "        \n",
    "        for i in range(len(data) - look_back):\n",
    "            x_arr.append(data[i:(i + look_back),0])\n",
    "            y_arr.append(data[i + look_back, 0])\n",
    "        \n",
    "        x_arr = np.array(x_arr)\n",
    "        x_arr = np.reshape(x_arr, (x_arr.shape[0], x_arr.shape[1],1))\n",
    "        \n",
    "        return x_arr, np.array(y_arr)\n",
    "    \n",
    "    #학습\n",
    "    def training(no):\n",
    "        #데이터 전처리 (0 ~ 1 범위)\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        #지금 전체 카운트롤 계산 했지만 Thread 문구류 각각해서 동시에 돌릴 예정\n",
    "        data_temp = (df['Stationery_NO'] == no)\n",
    "        data_result = df[data_temp]\n",
    "        print(data_result)\n",
    "        \n",
    "        temp = df[data_temp].values.reshape(-1,1)\n",
    "        trade_count = scaler.fit_transform(temp)\n",
    "        \n",
    "        #훈련\n",
    "        train = trade_count[0:int(len(trade_count) * 0.5)]\n",
    "        #검증\n",
    "        val = trade_count[int(len(trade_count) * 0.5) : int(len(trade_count) *0.75)]\n",
    "        #시험\n",
    "        test = trade_count[int(len(trade_count) * 0.75) : -1]\n",
    "        \n",
    "        #x,y 트레이닝 셋\n",
    "        x_train, y_train = AI_Model.create_dataset(train, 1)\n",
    "        #x,y 검증셋\n",
    "        x_val, y_val = AI_Model.create_dataset(val,1)\n",
    "        #x,y 시험 셋\n",
    "        x_test, y_text = AI_Model.create_dataset(val,1)\n",
    "        \n",
    "        #학습 모델 구성\n",
    "        batch_size = 22\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(LSTM(50, return_sequences=True, input_shape=(1,1)))\n",
    "        model.add(LSTM(64,return_sequences=False))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        #손실\n",
    "        model.compile(loss='mse', optimizer='rmsprop')\n",
    "        model.summary()\n",
    "        model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=50)\n",
    "        predictions = model.predict(x_test, batch_size)\n",
    "        read_predictions = scaler.inverse_transform(predictions)\n",
    "        result = read_predictions[-1]\n",
    "        \n",
    "        print(str(no) + \"의 문구류의 값은 \" + str(result) +\" 입니다.\")\n",
    "        \n",
    "        #학습결과_Dict\n",
    "        global result_dict\n",
    "        result_dict['Stationery_NO'].append(no)\n",
    "        result_dict['Count'].append(int(result))\n",
    "        result_now = str(now_result.tm_year) + \"-\" + str(now_result.tm_mon) + \"-\" + str(now_result.tm_mday)\n",
    "        result_dict['Today'].append(result_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#쓰레드 생성, 실행 함수\n",
    "def run_threads(list_temp):\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(len(list_temp)):\n",
    "        thread = t.Thread(target=AI_Model.training,kwargs={\"no\" : list_temp[i]})\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "select Stationery_NO, Count, MONTH(Date) as 'months' from ai_db.releases where Weekday = 1 and MONTH(Date) = 12;\n",
      "db_read 성공!\n"
     ]
    }
   ],
   "source": [
    "data_read(weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Count  Stationery_NO  months\n",
      "0         2              2      12\n",
      "13        1              2      12\n",
      "20        1              2      12\n",
      "21        1              2      12\n",
      "26       53              2      12\n",
      "36        6              2      12\n",
      "37        3              2      12\n",
      "41       45              2      12\n",
      "45       12              2      12\n",
      "52       29              2      12\n",
      "55       17              2      12\n",
      "69       16              2      12\n",
      "85       34              2      12\n",
      "90       53              2      12\n",
      "93        1              2      12\n",
      "107       5              2      12\n",
      "117      11              2      12\n",
      "119       1              2      12\n",
      "122       1              2      12\n",
      "127       1              2      12\n",
      "128       3              2      12\n",
      "131       3              2      12\n",
      "138      55              2      12\n",
      "163      12              2      12\n",
      "165      24              2      12\n",
      "169      15              2      12\n",
      "174       2              2      12\n",
      "180      44              2      12\n",
      "190       9              2      12\n",
      "191       1              2      12\n",
      "...     ...            ...     ...\n",
      "1104     27              2      12\n",
      "1106      6              2      12\n",
      "1107      1              2      12\n",
      "1108      1              2      12\n",
      "1112      5              2      12\n",
      "1117      1              2      12\n",
      "1118      2              2      12\n",
      "1126     24              2      12\n",
      "1130      4              2      12\n",
      "1135     31              2      12\n",
      "1146      1              2      12\n",
      "1161     66              2      12\n",
      "1191      5              2      12\n",
      "1193     12              2      12\n",
      "1196      1              2      12\n",
      "1197      1              2      12\n",
      "1204     35              2      12\n",
      "1206     17              2      12\n",
      "1207      2              2      12\n",
      "1208     73              2      12\n",
      "1211      1              2      12\n",
      "1222      3              2      12\n",
      "1225      4              2      12\n",
      "1231     25              2      12\n",
      "1232     25              2      12\n",
      "1235      2              2      12\n",
      "1241     13              2      12\n",
      "1242     10              2      12\n",
      "1248      1              2      12\n",
      "1252      6              2      12\n",
      "\n",
      "[199 rows x 3 columns]      Count  Stationery_NO  months\n",
      "3        38              1      12\n",
      "4         7              1      12\n",
      "11        2              1      12\n",
      "17        1              1      12\n",
      "28       21              1      12\n",
      "34       39              1      12\n",
      "38        5              1      12\n",
      "40        9              1      12\n",
      "46        7              1      12\n",
      "47       10              1      12\n",
      "50       10              1      12\n",
      "68       24              1      12\n",
      "73        2              1      12\n",
      "75        3              1      12\n",
      "76        1              1      12\n",
      "84       19              1      12\n",
      "96       27              1      12\n",
      "98        5              1      12\n",
      "105       5              1      12\n",
      "115       4              1      12\n",
      "145      12              1      12\n",
      "148      21              1      12\n",
      "149       3              1      12\n",
      "151       1              1      12\n",
      "182      21              1      12\n",
      "186      22              1      12\n",
      "188       1              1      12\n",
      "189       1              1      12\n",
      "198      43              1      12\n",
      "199      21              1      12\n",
      "...     ...            ...     ...\n",
      "1115      6              1      12\n",
      "1123     54              1      12\n",
      "1137      9              1      12\n",
      "1151     37              1      12\n",
      "1152     24              1      12\n",
      "1154      1              1      12\n",
      "1155      2              1      12\n",
      "1162      2              1      12\n",
      "1169     10              1      12\n",
      "1175      4              1      12\n",
      "1178      1              1      12\n",
      "1179      4              1      12\n",
      "1180      7              1      12\n",
      "1182     46              1      12\n",
      "1190     28              1      12\n",
      "1194      7              1      12\n",
      "1198      1              1      12\n",
      "1200      2              1      12\n",
      "1209     36              1      12\n",
      "1220     15              1      12\n",
      "1221      3              1      12\n",
      "1226      7              1      12\n",
      "1227      3              1      12\n",
      "1228      7              1      12\n",
      "1233      5              1      12\n",
      "1237      6              1      12\n",
      "1238      3              1      12\n",
      "1240      4              1      12\n",
      "1246      7              1      12\n",
      "1255     12              1      12\n",
      "\n",
      "[200 rows x 3 columns]\n",
      "\n",
      "      Count  Stationery_NO  months\n",
      "2        38              3      12\n",
      "10        7              3      12\n",
      "22       16              3      12\n",
      "23       18              3      12\n",
      "48        9              3      12\n",
      "49        5              3      12\n",
      "53        6              3      12\n",
      "54       24              3      12\n",
      "59        6              3      12\n",
      "60       19              3      12\n",
      "61        7              3      12\n",
      "62        2              3      12\n",
      "63        3              3      12\n",
      "72       25              3      12\n",
      "74        4              3      12\n",
      "79        1              3      12\n",
      "83        2              3      12\n",
      "88        1              3      12\n",
      "89        3              3      12\n",
      "94       35              3      12\n",
      "100      12              3      12\n",
      "103       2              3      12\n",
      "112      44              3      12\n",
      "113       2              3      12\n",
      "129      19              3      12\n",
      "136       1              3      12\n",
      "140       3              3      12\n",
      "141       5              3      12\n",
      "144       1              3      12\n",
      "147       1              3      12\n",
      "...     ...            ...     ...\n",
      "1105      1              3      12\n",
      "1109      4              3      12\n",
      "1110     32              3      12\n",
      "1111      2              3      12\n",
      "1132      6              3      12\n",
      "1134     49              3      12\n",
      "1138      3              3      12\n",
      "1144      2              3      12\n",
      "1145      3              3      12\n",
      "1149      2              3      12\n",
      "1150      1              3      12\n",
      "1160     45              3      12\n",
      "1171      1              3      12\n",
      "1173      2              3      12\n",
      "1176      1              3      12\n",
      "1183      4              3      12\n",
      "1185     12              3      12\n",
      "1216      6              3      12\n",
      "1219      1              3      12\n",
      "1230      4              3      12\n",
      "1243     46              3      12\n",
      "1245      2              3      12\n",
      "1250     13              3      12\n",
      "1251      7              3      12\n",
      "1253      6              3      12\n",
      "1254      3              3      12\n",
      "1256      1              3      12\n",
      "1270      9              3      12\n",
      "1275      3              3      12\n",
      "1276     21              3      12\n",
      "\n",
      "[224 rows x 3 columns]\n",
      "      Count  Stationery_NO  months\n",
      "5         4              4      12\n",
      "9        42              4      12\n",
      "14        3              4      12\n",
      "16        3              4      12\n",
      "24       44              4      12\n",
      "27        4              4      12\n",
      "35        3              4      12\n",
      "39       25              4      12\n",
      "43       15              4      12\n",
      "56       10              4      12\n",
      "57       14              4      12\n",
      "58        3              4      12\n",
      "64        1              4      12\n",
      "65        1              4      12\n",
      "66        1              4      12\n",
      "67       26              4      12\n",
      "80        2              4      12\n",
      "87        3              4      12\n",
      "91       45              4      12\n",
      "95        2              4      12\n",
      "106       2              4      12\n",
      "109       1              4      12\n",
      "110       3              4      12\n",
      "116       3              4      12\n",
      "124       3              4      12\n",
      "126       1              4      12\n",
      "132      15              4      12\n",
      "134       8              4      12\n",
      "137       3              4      12\n",
      "146       1              4      12\n",
      "...     ...            ...     ...\n",
      "1097      1              4      12\n",
      "1101      3              4      12\n",
      "1102      3              4      12\n",
      "1113     29              4      12\n",
      "1116      3              4      12\n",
      "1119      7              4      12\n",
      "1128     43              4      12\n",
      "1140     42              4      12\n",
      "1148      5              4      12\n",
      "1163     77              4      12\n",
      "1167      5              4      12\n",
      "1168      4              4      12\n",
      "1172      1              4      12\n",
      "1192     72              4      12\n",
      "1195      8              4      12\n",
      "1199      7              4      12\n",
      "1202      3              4      12\n",
      "1214      1              4      12\n",
      "1215      2              4      12\n",
      "1217      3              4      12\n",
      "1223      1              4      12\n",
      "1229     29              4      12\n",
      "1249     10              4      12\n",
      "1258      2              4      12\n",
      "1259      3              4      12\n",
      "1260      1              4      12\n",
      "1265     54              4      12\n",
      "1267      8              4      12\n",
      "1268      1              4      12\n",
      "1269      2              4      12\n",
      "\n",
      "[224 rows x 3 columns]      Count  Stationery_NO  months\n",
      "1         5              5      12\n",
      "6         3              5      12\n",
      "8        21              5      12\n",
      "30        2              5      12\n",
      "42       69              5      12\n",
      "44        1              5      12\n",
      "81        3              5      12\n",
      "82        1              5      12\n",
      "97       32              5      12\n",
      "99        8              5      12\n",
      "101       6              5      12\n",
      "102       2              5      12\n",
      "111      31              5      12\n",
      "121      25              5      12\n",
      "123       1              5      12\n",
      "125       1              5      12\n",
      "130      12              5      12\n",
      "133      21              5      12\n",
      "143       8              5      12\n",
      "157      13              5      12\n",
      "158       2              5      12\n",
      "159       2              5      12\n",
      "167      45              5      12\n",
      "170       3              5      12\n",
      "184       1              5      12\n",
      "192       6              5      12\n",
      "197       1              5      12\n",
      "201      58              5      12\n",
      "202       8              5      12\n",
      "203       1              5      12\n",
      "...     ...            ...     ...\n",
      "1129     51              5      12\n",
      "1139      3              5      12\n",
      "1141      1              5      12\n",
      "1143      1              5      12\n",
      "1147      1              5      12\n",
      "1157     43              5      12\n",
      "1159      1              5      12\n",
      "1164      7              5      12\n",
      "1165      7              5      12\n",
      "1170      2              5      12\n",
      "1174      1              5      12\n",
      "1184     29              5      12\n",
      "1187     20              5      12\n",
      "1201     76              5      12\n",
      "1203      3              5      12\n",
      "1205      4              5      12\n",
      "1212     10              5      12\n",
      "1234     52              5      12\n",
      "1236      3              5      12\n",
      "1239      1              5      12\n",
      "1247      1              5      12\n",
      "1257     22              5      12\n",
      "1261      1              5      12\n",
      "1262      1              5      12\n",
      "1263      2              5      12\n",
      "1271     38              5      12\n",
      "1274     23              5      12\n",
      "1277      5              5      12\n",
      "1278      1              5      12\n",
      "1279      1              5      12\n",
      "\n",
      "[208 rows x 3 columns]\n",
      "\n",
      "      Count  Stationery_NO  months\n",
      "7        28              6      12\n",
      "12        6              6      12\n",
      "15        3              6      12\n",
      "18        2              6      12\n",
      "19        1              6      12\n",
      "25       24              6      12\n",
      "29       42              6      12\n",
      "31        4              6      12\n",
      "32        5              6      12\n",
      "33        2              6      12\n",
      "51        2              6      12\n",
      "70       12              6      12\n",
      "71       13              6      12\n",
      "77       20              6      12\n",
      "78        3              6      12\n",
      "86        3              6      12\n",
      "92        5              6      12\n",
      "104       8              6      12\n",
      "108      11              6      12\n",
      "114       2              6      12\n",
      "118       8              6      12\n",
      "120       2              6      12\n",
      "135      26              6      12\n",
      "139      33              6      12\n",
      "142      19              6      12\n",
      "150       1              6      12\n",
      "156      10              6      12\n",
      "160       2              6      12\n",
      "161       2              6      12\n",
      "162       2              6      12\n",
      "...     ...            ...     ...\n",
      "1066      1              6      12\n",
      "1071     72              6      12\n",
      "1074      6              6      12\n",
      "1088     14              6      12\n",
      "1121      3              6      12\n",
      "1124     55              6      12\n",
      "1125     12              6      12\n",
      "1127      8              6      12\n",
      "1131      1              6      12\n",
      "1133      1              6      12\n",
      "1136     46              6      12\n",
      "1142     10              6      12\n",
      "1153     25              6      12\n",
      "1156     13              6      12\n",
      "1158     14              6      12\n",
      "1166      8              6      12\n",
      "1177      4              6      12\n",
      "1181      1              6      12\n",
      "1186     55              6      12\n",
      "1188      9              6      12\n",
      "1189      3              6      12\n",
      "1210     33              6      12\n",
      "1213     11              6      12\n",
      "1218      2              6      12\n",
      "1224      3              6      12\n",
      "1244     28              6      12\n",
      "1264      6              6      12\n",
      "1266      7              6      12\n",
      "1272     36              6      12\n",
      "1273      4              6      12\n",
      "\n",
      "[225 rows x 3 columns]\n",
      "WARNING:tensorflow:From C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 299 samples, validate on 149 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 297 samples, validate on 148 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 311 samples, validate on 155 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 335 samples, validate on 167 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 335 samples, validate on 167 samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 39,905\n",
      "Trainable params: 39,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 336 samples, validate on 168 samples\n",
      "WARNING:tensorflow:From C:\\Users\\kkag2\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "Epoch 1/50Epoch 1/50\n",
      "\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "311/311 [==============================] - 6s 20ms/sample - loss: 0.0247 - val_loss: 0.0228\n",
      "Epoch 2/50\n",
      "336/336 [==============================] - 6s 19ms/sample - loss: 0.0164 - val_loss: 0.0218\n",
      "Epoch 2/50\n",
      "311/311 [==============================] - 0s 1ms/sample - loss: 0.0187 - val_loss: 0.0212\n",
      "Epoch 3/50\n",
      "336/336 [==============================] - 0s 1ms/sample - loss: 0.0118 - val_loss: 0.0209\n",
      "Epoch 3/50\n",
      "297/297 [==============================] - 7s 23ms/sample - loss: 0.0237 - val_loss: 0.0187\n",
      "Epoch 2/50\n",
      "311/311 [==============================] - 0s 875us/sample - loss: 0.0182 - val_loss: 0.0219\n",
      "Epoch 4/50\n",
      "336/336 [==============================] - 0s 967us/sample - loss: 0.0118 - val_loss: 0.0211\n",
      "Epoch 4/50\n",
      "299/299 [==============================] - 7s 24ms/sample - loss: 0.0174 - val_loss: 0.0228\n",
      "Epoch 2/50\n",
      "335/335 [==============================] - 7s 22ms/sample - loss: 0.0145 - val_loss: 0.0258\n",
      "Epoch 2/50\n",
      "335/335 [==============================] - 7s 21ms/sample - loss: 0.0125 - val_loss: 0.0268\n",
      "Epoch 2/50\n",
      "297/297 [==============================] - 0s 1ms/sample - loss: 0.0199 - val_loss: 0.0170\n",
      "Epoch 3/50\n",
      "311/311 [==============================] - ETA: 0s - loss: 0.008 - 1s 2ms/sample - loss: 0.0184 - val_loss: 0.0211\n",
      "Epoch 5/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0148 - val_loss: 0.0223\n",
      "110/297 [==========>...................] - ETA: 0s - loss: 0.0269Epoch 3/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0210\n",
      "154/297 [==============>...............] - ETA: 0s - loss: 0.0223Epoch 5/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0257\n",
      "Epoch 3/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0105 - val_loss: 0.0259\n",
      "Epoch 3/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0194 - val_loss: 0.0169================>........] - ETA: 0s - loss: \n",
      "198/335 [================>.............] - ETA: 0s - loss: 0.0084Epoch 4/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0147 - val_loss: 0.0222\n",
      "Epoch 4/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0211\n",
      "Epoch 6/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0266\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0218\n",
      "Epoch 4/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0261\n",
      "Epoch 6/50\n",
      "Epoch 4/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0194 - val_loss: 0.0169\n",
      "Epoch 5/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0146 - val_loss: 0.0221\n",
      "Epoch 5/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0183 - val_loss: 0.0210\n",
      "Epoch 7/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0105 - val_loss: 0.0266\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0262\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0207\n",
      "Epoch 5/50\n",
      "Epoch 7/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0193 - val_loss: 0.0167\n",
      "Epoch 6/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0144 - val_loss: 0.0220\n",
      "Epoch 6/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0210\n",
      "Epoch 8/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0265\n",
      "154/311 [=============>................] - ETA: 0s - loss: 0.0250Epoch 6/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0260\n",
      "Epoch 6/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0167\n",
      "Epoch 7/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0118 - val_loss: 0.0210\n",
      "Epoch 8/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0143 - val_loss: 0.0222\n",
      "Epoch 7/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0210\n",
      "Epoch 9/50\n",
      "297/297 [==============================] - 0s 1ms/sample - loss: 0.0192 - val_loss: 0.0167\n",
      "330/336 [============================>.] - ETA: 0s - loss: 0.0119Epoch 8/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0273\n",
      "Epoch 7/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0254\n",
      "Epoch 7/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0208\n",
      "Epoch 9/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0143 - val_loss: 0.0223\n",
      "Epoch 8/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0210\n",
      "Epoch 10/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0193 - val_loss: 0.0166\n",
      "Epoch 9/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0254\n",
      "Epoch 8/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0250\n",
      "198/311 [==================>...........] - ETA: 0s - loss: 0.0179Epoch 8/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0143 - val_loss: 0.0221\n",
      "Epoch 9/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0209\n",
      "Epoch 10/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0211..............] - ETA: \n",
      "Epoch 11/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0167\n",
      "Epoch 10/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 10/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0259\n",
      "198/311 [==================>...........] - ETA: 0s - loss: 0.0213Epoch 9/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0254\n",
      "Epoch 9/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0211\n",
      "Epoch 11/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0211\n",
      "Epoch 12/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0167\n",
      "Epoch 11/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0143 - val_loss: 0.0220\n",
      "Epoch 11/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0264\n",
      "Epoch 10/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0257\n",
      "Epoch 10/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0213\n",
      "242/297 [=======================>......] - ETA: 0s - loss: 0.0179Epoch 12/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 12/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0212\n",
      "Epoch 13/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0143 - val_loss: 0.0221>.............] - ETA: 0s - loss\n",
      "Epoch 12/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0263========>.....] - ETA: 0s - loss: \n",
      "Epoch 11/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0249\n",
      "Epoch 11/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0207\n",
      "286/311 [==========================>...] - ETA: 0s - loss: 0.0177\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 13/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0214\n",
      "Epoch 14/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0218..............] - ETA: 0s - loss: \n",
      "Epoch 13/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0256=======>......] - ETA: 0s - loss: \n",
      "Epoch 12/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0268\n",
      "Epoch 12/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 14/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0118 - val_loss: 0.0205\n",
      "Epoch 14/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0212\n",
      "Epoch 15/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0219\n",
      "Epoch 14/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0261\n",
      "Epoch 13/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0168\n",
      "Epoch 15/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0270\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0065Epoch 13/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0209\n",
      " 66/335 [====>.........................] - ETA: 0s - loss: 0.0073Epoch 15/50\n",
      "110/335 [========>.....................] - ETA: 0s - loss: 0.0088 - ETA: 0s - loss: 0.0035311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0211\n",
      "Epoch 16/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0221\n",
      "Epoch 15/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.01665 [================>.............] - ET\n",
      "Epoch 16/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0249\n",
      "Epoch 14/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0102 - val_loss: 0.0259\n",
      "Epoch 14/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0223\n",
      "Epoch 16/5\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 17/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 16/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0193 - val_loss: 0.0166\n",
      "Epoch 17/50\n",
      "299/299 [==============================] - 0s 1ms/sample - loss: 0.0143 - val_loss: 0.0219============>.] - ETA: 0s - loss: \n",
      "Epoch 17/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0262\n",
      "Epoch 15/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0258\n",
      "Epoch 15/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0180 - val_loss: 0.0213..............] - ETA: 0s - loss: 0.0\n",
      "Epoch 18/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0212\n",
      "Epoch 17/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0168..............] - ET\n",
      "Epoch 18/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0218\n",
      "Epoch 18/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0251\n",
      "Epoch 16/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0277\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0210\n",
      "Epoch 19/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0206\n",
      "154/299 [==============>...............] - ETA: 0s - loss: 0.0143Epoch 18/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 19/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0219>.............] - ETA: 0s - loss: 0.006 - ETA: 0s - lo\n",
      "Epoch 19/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0252\n",
      "Epoch 17/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0210\n",
      "Epoch 20/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0263\n",
      "Epoch 17/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0207 - loss: 0.0098 - ETA: 0s - loss: \n",
      "Epoch 19/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0169\n",
      "Epoch 20/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 20/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0253========>.....] - ETA: 0s - loss: 0.012\n",
      "Epoch 18/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0267\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0210\n",
      "Epoch 18/50\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0043Epoch 21/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0190 - val_loss: 0.0168\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0217\n",
      "Epoch 21/50\n",
      "Epoch 20/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0140 - val_loss: 0.0227..............] - ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 21/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0180 - val_loss: 0.0209..............] - ETA: 0s\n",
      "Epoch 22/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0261\n",
      "Epoch 19/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0257\n",
      "Epoch 19/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0193 - val_loss: 0.0166\n",
      " 66/311 [=====>........................] - ETA: 0s - loss: 0.0144\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0118 - val_loss: 0.0210\n",
      "110/335 [========>.....................] - ETA: 0s - loss: 0.0131Epoch 21/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0221\n",
      "Epoch 22/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0214154/299 [==============>...............] - ETA: 0s - loss: 0.\n",
      "Epoch 23/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0190 - val_loss: 0.0168\n",
      "Epoch 23/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0251\n",
      " 22/311 [=>............................] - ETA: 0s - loss: 0.0372Epoch 20/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0262\n",
      "Epoch 20/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0207\n",
      "Epoch 22/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0221\n",
      "110/335 [========>.....................] - ETA: 0s - loss: 0.0102Epoch 23/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0193 - val_loss: 0.0166\n",
      "Epoch 24/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0210\n",
      "Epoch 24/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0250\n",
      "Epoch 21/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0270\n",
      "Epoch 21/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0219\n",
      "Epoch 24/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0208\n",
      " 22/299 [=>............................] - ETA: 0s - loss: 0.0095Epoch 23/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 25/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0211\n",
      "Epoch 25/50\n",
      "335/335 [==============================] - 1s 1ms/sample - loss: 0.0103 - val_loss: 0.0266\n",
      "Epoch 22/5\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0262\n",
      "Epoch 22/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0221\n",
      "Epoch 25/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0206\n",
      "Epoch 24/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 26/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0210\n",
      " 66/297 [=====>........................] - ETA: 0s - loss: 0.0146Epoch 26/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0263\n",
      " 66/311 [=====>........................] - ETA: 0s - loss: 0.0270Epoch 23/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0220\n",
      "Epoch 26/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0254\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0029Epoch 23/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 25/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 27/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 27/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0220\n",
      "Epoch 27/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0262\n",
      "Epoch 24/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0254\n",
      "Epoch 24/50\n",
      "198/311 [==================>...........] - ETA: 0s - loss: 0.0108 - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0206\n",
      "110/335 [========>.....................] - ETA: 0s - loss: 0.006Epoch 26/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 28/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0180 - val_loss: 0.0214\n",
      "Epoch 28/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0222\n",
      "Epoch 28/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0257\n",
      "Epoch 25/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0262\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0057Epoch 25/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0168\n",
      "Epoch 29/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0206\n",
      "198/299 [==================>...........] - ETA: 0s - loss: 0.0142Epoch 27/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0211\n",
      "Epoch 29/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0220\n",
      "Epoch 29/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0271\n",
      "Epoch 26/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0255\n",
      "Epoch 26/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0167\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0027Epoch 30/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0206====>.........] - ETA: 0s - loss: 0.0\n",
      "Epoch 28/50\n",
      " - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0220 - loss: 0.011\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0180 - val_loss: 0.0216\n",
      "Epoch 30/50\n",
      "Epoch 30/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0104 - val_loss: 0.0269\n",
      "Epoch 27/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0257\n",
      "Epoch 27/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0209\n",
      "Epoch 29/50\n",
      "299/299 [==============================] - 0s 1ms/sample - loss: 0.0141 - val_loss: 0.0218\n",
      "Epoch 31/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0180 - val_loss: 0.0210\n",
      "110/336 [========>.....................] - ETA: 0s - loss: 0.0089Epoch 31/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 32/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0254\n",
      "Epoch 28/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0263\n",
      "Epoch 28/50\n",
      "299/299 [==============================] - 0s 1ms/sample - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 32/5\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0207\n",
      "Epoch 30/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0179 - val_loss: 0.0214\n",
      "Epoch 32/50\n",
      "297/297 [==============================] - 0s 1ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 33/50\n",
      "335/335 [==============================] - 0s 1ms/sample - loss: 0.0103 - val_loss: 0.0262========>.....] - ETA: 0s - loss: 0. 66/297 [=====>........................] - ETA: 0s - loss: 0.03\n",
      "Epoch 29/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0257.............] - ETA: 0s - loss: 0.026\n",
      "Epoch 29/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0221\n",
      "Epoch 33/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0210\n",
      "Epoch 31/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 33/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 34/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0270 - loss: 0.0106 - ETA: 0s -\n",
      "Epoch 30/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0258\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0084Epoch 30/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0220\n",
      "Epoch 34/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0212\n",
      "110/335 [========>.....................] - ETA: 0s - loss: 0.0113Epoch 32/5\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0210..............] - ETA: 0s - loss: 0.008\n",
      "Epoch 34/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 35/50\n",
      "299/299 [==============================] - 0s 1ms/sample - loss: 0.0142 - val_loss: 0.0219 - ETA: 0s - lo\n",
      "Epoch 35/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0257\n",
      "242/311 [======================>.......] - ETA: 0s - loss: 0.0163Epoch 31/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0260\n",
      "Epoch 31/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0206\n",
      "Epoch 33/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0209\n",
      "Epoch 35/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "Epoch 36/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0218.............] - ETA: 0s - loss: 0.0\n",
      "Epoch 36/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0257\n",
      "Epoch 32/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0259\n",
      "Epoch 32/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 37/5\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 36/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0207\n",
      "Epoch 34/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0218\n",
      "Epoch 37/50\n",
      "335/335 [==============================] - 0s 1ms/sample - loss: 0.0113 - val_loss: 0.0249\n",
      "Epoch 33/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0268\n",
      "Epoch 33/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 38/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0213\n",
      "Epoch 37/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0206\n",
      "Epoch 35/50\n",
      "299/299 [==============================] - 0s 1ms/sample - loss: 0.0142 - val_loss: 0.0219\n",
      "Epoch 38/50\n",
      "335/335 [============================== - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0251=====>.........] - ETA: 0s - loss: 0.012\n",
      "154/299 [==============>...............] - ETA: 0s - loss: 0.0184Epoch 34/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0266\n",
      "Epoch 34/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0190 - val_loss: 0.0165\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0080Epoch 39/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0210\n",
      " 66/335 [====>.........................] - ETA: 0s - loss: 0.0099Epoch 38/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0213\n",
      "110/297 [==========>...................] - ETA: 0s - loss: 0.0121\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0222\n",
      "Epoch 39/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 40/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0252\n",
      "Epoch 35/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0102 - val_loss: 0.0256\n",
      "Epoch 35/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0180 - val_loss: 0.0209\n",
      "264/299 [=========================>....] - ETA: 0s - loss: 0.0150\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0117 - val_loss: 0.0209\n",
      "Epoch 37/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0221\n",
      " 22/336 [>.............................] - ETA: 0s - loss: 0.0251Epoch 40/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.01669 [==============>...............] - ETA: 0s - loss: 0\n",
      "Epoch 41/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0251\n",
      "Epoch 36/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0262\n",
      "Epoch 36/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0182 - val_loss: 0.0211\n",
      "Epoch 40/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0219\n",
      "Epoch 41/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0207\n",
      "Epoch 38/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0165 [================>.............] - ETA: 0s -\n",
      "198/299 [==================>...........] - ETA: 0s - loss: 0.0130Epoch 42/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0210\n",
      "Epoch 41/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0271\n",
      "Epoch 37/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0256\n",
      "Epoch 37/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0218\n",
      "Epoch 42/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0118 - val_loss: 0.0206\n",
      "Epoch 39/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0166\n",
      "242/335 [====================>.........] - ETA: 0s - loss: 0.0114Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0212.............] - ETA: 0s - loss: \n",
      "Epoch 42/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0222\n",
      "Epoch 43/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0114 - val_loss: 0.0256\n",
      "Epoch 38/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0261\n",
      "Epoch 38/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0210\n",
      "286/297 [===========================>..] - ETA: 0s - loss: 0.0197Epoch 40/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 44/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 43/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0143 - val_loss: 0.0221\n",
      "Epoch 44/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0255\n",
      "Epoch 39/50\n",
      "286/336 [========================>.....] - ETA: 0s - loss: 0.0124335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0250\n",
      "Epoch 39/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0210\n",
      "Epoch 41/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0190 - val_loss: 0.0165\n",
      "Epoch 45/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0218\n",
      "Epoch 45/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 44/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0272\n",
      "Epoch 40/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0252\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.0025\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0167\n",
      "Epoch 46/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0209\n",
      "Epoch 42/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0220=========>....] - ETA: 0s - loss\n",
      "Epoch 46/50\n",
      "311/311 [==============================] - 1s 2ms/sample - loss: 0.0181 - val_loss: 0.0215\n",
      "Epoch 45/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0252\n",
      "Epoch 41/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0259\n",
      "Epoch 41/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0192 - val_loss: 0.0165\n",
      "Epoch 47/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0207\n",
      "Epoch 43/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0222\n",
      "Epoch 47/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0212\n",
      "242/297 [=======================>......] - ETA: 0s - loss: 0.0166Epoch 46/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 48/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0250\n",
      "Epoch 42/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0258\n",
      "Epoch 42/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0207\n",
      "Epoch 44/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0141 - val_loss: 0.0219\n",
      "Epoch 48/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0209..............] - ETA: 0s - loss:\n",
      "Epoch 47/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 49/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0254\n",
      "Epoch 43/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0103 - val_loss: 0.0266\n",
      "Epoch 43/50\n",
      "299/299 [==============================] - 0s 2ms/sample - loss: 0.0142 - val_loss: 0.0219\n",
      "Epoch 49/50\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0116 - val_loss: 0.0207\n",
      "Epoch 45/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 48/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      " 66/311 [=====>........................] - ETA: 0s - loss: 0.0131Epoch 50/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0113 - val_loss: 0.0259..............] - ETA: 0s - loss:\n",
      "Epoch 44/50\n",
      "299/299 [==============================] - ETA: 0s - loss: 0.014 - 0s 1ms/sample - loss: 0.0141 - val_loss: 0.0218\n",
      "Epoch 50/50\n",
      "335/335 [==============================] - 1s 2ms/sample - loss: 0.0102 - val_loss: 0.0260\n",
      "330/336 [============================>.] - ETA: 0s - loss: 0.0117Epoch 44/5\n",
      "336/336 [==============================] - 1s 2ms/sample - loss: 0.0115 - val_loss: 0.0212\n",
      "Epoch 46/50\n",
      "311/311 [==============================] - 0s 2ms/sample - loss: 0.0182 - val_loss: 0.0211==========>..] - ETA: 0s - loss: 0.\n",
      "Epoch 49/50\n",
      "297/297 [==============================] - 0s 2ms/sample - loss: 0.0191 - val_loss: 0.0166\n",
      "299/299 [==============================] - 0s 1ms/sample - loss: 0.0140 - val_loss: 0.0224\n",
      "335/335 [==============================] - 0s 1ms/sample - loss: 0.0113 - val_loss: 0.0259\n",
      "Epoch 45/50\n",
      "335/335 [==============================] - 0s 1ms/sample - loss: 0.0103 - val_loss: 0.0265\n",
      "Epoch 45/50\n",
      "336/336 [==============================] - 0s 1ms/sample - loss: 0.0115 - val_loss: 0.0206\n",
      "Epoch 47/50\n",
      "311/311 [==============================] - 0s 1ms/sample - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 50/50\n",
      "220/335 [==================>...........] - ETA: 0s - loss: 0.01082의 문구류의 값은 [10.921663] 입니다.\n",
      "220/311 [====================>.........] - ETA: 0s - loss: 0.01761의 문구류의 값은 [8.362738] 입니다.\n",
      "335/335 [==============================] - 0s 1ms/sample - loss: 0.0113 - val_loss: 0.0261\n",
      "Epoch 46/50\n",
      "335/335 [==============================] - 0s 1ms/sample - loss: 0.0103 - val_loss: 0.0267\n",
      "Epoch 46/50\n",
      "336/336 [==============================] - 0s 1ms/sample - loss: 0.0116 - val_loss: 0.0205\n",
      "Epoch 48/50\n",
      "311/311 [==============================] - 0s 1ms/sample - loss: 0.0182 - val_loss: 0.0210\n",
      "335/335 [==============================] - 0s 915us/sample - loss: 0.0113 - val_loss: 0.0250...........] - ETA: 0s - loss: 0\n",
      "Epoch 47/50\n",
      "335/335 [==============================] - 0s 934us/sample - loss: 0.0103 - val_loss: 0.0255\n",
      "Epoch 47/50\n",
      " 22/335 [>.............................] - ETA: 0s - loss: 0.010336/336 [==============================] - 0s 888us/sample - loss: 0.0116 - val_loss: 0.0209\n",
      "110/335 [========>.....................] - ETA: 0s - loss: 0.0128Epoch 49/50\n",
      "176/335 [==============>...............] - ETA: 0s - loss: 0.0065의 문구류의 값은 [10.164213] 입니다.\n",
      "335/335 [==============================] - 0s 908us/sample - loss: 0.0113 - val_loss: 0.0254\n",
      "Epoch 48/50\n",
      "335/335 [==============================] - 0s 905us/sample - loss: 0.0103 - val_loss: 0.0261\n",
      "Epoch 48/50\n",
      "336/336 [==============================] - 0s 911us/sample - loss: 0.0115 - val_loss: 0.0205\n",
      "Epoch 50/50\n",
      "335/335 [==============================] - 0s 884us/sample - loss: 0.0113 - val_loss: 0.0266\n",
      "Epoch 49/50\n",
      "335/335 [==============================] - 0s 881us/sample - loss: 0.0103 - val_loss: 0.0257\n",
      "Epoch 49/50\n",
      "336/336 [==============================] - 0s 872us/sample - loss: 0.0116 - val_loss: 0.0205\n",
      "335/335 [==============================] - 0s 737us/sample - loss: 0.0113 - val_loss: 0.0263\n",
      "Epoch 50/50\n",
      "335/335 [==============================] - 0s 711us/sample - loss: 0.0104 - val_loss: 0.0262\n",
      "Epoch 50/50\n",
      "220/335 [==================>...........] - ETA: 0s - loss: 0.01126의 문구류의 값은 [10.095307] 입니다.\n",
      "335/335 [==============================] - 0s 687us/sample - loss: 0.0114 - val_loss: 0.0254\n",
      "335/335 [==============================] - 0s 675us/sample - loss: 0.0103 - val_loss: 0.0253\n",
      "4의 문구류의 값은 [9.345368] 입니다.\n",
      "3의 문구류의 값은 [10.50888] 입니다.\n",
      "thread time :  47.20584011077881\n"
     ]
    }
   ],
   "source": [
    "#모델 객체 생성\n",
    "ai_model = AI_Model()\n",
    "\n",
    "list_temp = list(Stationery_list)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#학습 모델 쓰레드 생성\n",
    "run_threads(list_temp)\n",
    "\n",
    "#쓰레드 미생성\n",
    "# for i in range(len(list_temp)):\n",
    "#     ai_model.training(list_temp[i])\n",
    "\n",
    "print('thread time : ', time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stationery_NO</th>\n",
       "      <th>Count</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-12-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-12-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-12-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-12-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-12-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2019-12-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Stationery_NO  Count      Today\n",
       "id                                 \n",
       "1               2     10  2019-12-2\n",
       "2               1      8  2019-12-2\n",
       "3               5     10  2019-12-2\n",
       "4               6     10  2019-12-2\n",
       "5               4      9  2019-12-2\n",
       "6               3     10  2019-12-2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result_dict)\n",
    "result_df.index +=1\n",
    "result_df.index.names = ['id']\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 데이터 업데이트 완료!\n"
     ]
    }
   ],
   "source": [
    "db_result = DB_SQL()\n",
    "\n",
    "db_result.db_write(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
